{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**LEITURA DOS DADOS DO SUSTAIN BENCH**"
      ],
      "metadata": {
        "id": "ppkckTGwFLH7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "nLuY6bhQDxmP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a807dde-9b53-4d76-f066-9cd0a20a0cf4",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shapes dos arrays:\n",
            "Hist: (247, 32, 32, 9)\n",
            "NDVI: (247, 32)\n",
            "Locs: (247,)\n",
            "Years: (247,)\n",
            "Keys: (247,)\n",
            "Yields: (247,)\n",
            "\n",
            "Primeiras linhas:\n",
            "     hist_0    hist_1    hist_2   hist_3    hist_4    hist_5    hist_6  \\\n",
            "0  0.000000  0.000270  0.000025  0.00000  0.000037  0.000061  0.000061   \n",
            "1  0.000000  0.000945  0.000000  0.00000  0.000065  0.000126  0.000189   \n",
            "2  0.000673  0.000657  0.000000  0.00003  0.000547  0.000437  0.002556   \n",
            "3  0.000000  0.000538  0.000107  0.00000  0.000401  0.000000  0.000000   \n",
            "4  0.000000  0.000343  0.000000  0.00000  0.000144  0.000274  0.000480   \n",
            "\n",
            "   hist_7  hist_8    hist_9  ...   ndvi_27   ndvi_28   ndvi_29   ndvi_30  \\\n",
            "0     0.0     0.0  0.000000  ...  0.698605  0.640461  0.566693  0.645517   \n",
            "1     0.0     0.0  0.000000  ...  0.822922  0.786009  0.720077  0.574686   \n",
            "2     0.0     0.0  0.000485  ...  0.637951  0.673795  0.624429  0.531740   \n",
            "3     0.0     0.0  0.000000  ...  0.729741  0.787198  0.824194  0.805108   \n",
            "4     0.0     0.0  0.000000  ...  0.699269  0.692859  0.646264  0.760457   \n",
            "\n",
            "    ndvi_31  lat_original  lon_original  year  \\\n",
            "0  0.605391          None          None  2008   \n",
            "1  0.662463          None          None  2011   \n",
            "2  0.597713          None          None  2007   \n",
            "3  0.793112          None          None  2011   \n",
            "4  0.579717          None          None  2011   \n",
            "\n",
            "                                     region_id  yield  \n",
            "0  triangulo mineiroalto paranaiba_brasil_2008  2.941  \n",
            "1                            assis_brasil_2011  3.100  \n",
            "2                       sul goiano_brasil_2007  2.737  \n",
            "3    centro ocidental riograndense_brasil_2011  2.753  \n",
            "4            oriental do tocantins_brasil_2011  3.030  \n",
            "\n",
            "[5 rows x 9253 columns]\n",
            "\n",
            "Colunas totais:\n",
            "Index(['hist_0', 'hist_1', 'hist_2', 'hist_3', 'hist_4', 'hist_5', 'hist_6',\n",
            "       'hist_7', 'hist_8', 'hist_9',\n",
            "       ...\n",
            "       'ndvi_27', 'ndvi_28', 'ndvi_29', 'ndvi_30', 'ndvi_31', 'lat_original',\n",
            "       'lon_original', 'year', 'region_id', 'yield'],\n",
            "      dtype='object', length=9253)\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# -----------------------------\n",
        "# Função para carregar um .npz\n",
        "# -----------------------------\n",
        "def load_npz_array(path):\n",
        "    data = np.load(path, allow_pickle=True)\n",
        "    return data[data.files[0]]\n",
        "\n",
        "# -----------------------------\n",
        "# 1. Carregar todos os arquivos de treino\n",
        "# -----------------------------\n",
        "X_hist = load_npz_array(\"train_hists.npz\")\n",
        "X_ndvi = load_npz_array(\"train_ndvi.npz\")\n",
        "X_loc = load_npz_array(\"train_locs.npz\")\n",
        "X_year = load_npz_array(\"train_years.npz\")\n",
        "X_key = load_npz_array(\"train_keys.npz\")\n",
        "y = load_npz_array(\"train_yields.npz\")\n",
        "\n",
        "# -----------------------------\n",
        "# 2. Mostrar shapes\n",
        "# -----------------------------\n",
        "print(\"Shapes dos arrays:\")\n",
        "print(\"Hist:\", X_hist.shape)\n",
        "print(\"NDVI:\", X_ndvi.shape)\n",
        "print(\"Locs:\", X_loc.shape)\n",
        "print(\"Years:\", X_year.shape)\n",
        "print(\"Keys:\", X_key.shape)\n",
        "print(\"Yields:\", y.shape)\n",
        "\n",
        "# -----------------------------\n",
        "# 3. Montar um DataFrame para visualização\n",
        "# -----------------------------\n",
        "\n",
        "# Hist tem muitas colunas → renomear\n",
        "X_hist_reshaped = X_hist.reshape(X_hist.shape[0], -1)\n",
        "hist_cols = [f\"hist_{i}\" for i in range(X_hist_reshaped.shape[1])]\n",
        "df_hist = pd.DataFrame(X_hist_reshaped, columns=hist_cols)\n",
        "\n",
        "# NDVI também\n",
        "ndvi_cols = [f\"ndvi_{i}\" for i in range(X_ndvi.shape[1])]\n",
        "df_ndvi = pd.DataFrame(X_ndvi, columns=ndvi_cols)\n",
        "\n",
        "# Locs - CORRIGIDO: cria duas colunas separadas\n",
        "df_loc = pd.DataFrame({'lat_original': [None]*len(X_loc), 'lon_original': [None]*len(X_loc)})\n",
        "\n",
        "df_year = pd.DataFrame(X_year, columns=[\"year\"])\n",
        "df_key = pd.DataFrame(X_key, columns=[\"region_id\"])\n",
        "df_y = pd.DataFrame(y, columns=[\"yield\"])\n",
        "\n",
        "# Junta tudo\n",
        "df = pd.concat([df_hist, df_ndvi, df_loc, df_year, df_key, df_y], axis=1)\n",
        "\n",
        "print(\"\\nPrimeiras linhas:\")\n",
        "print(df.head())\n",
        "\n",
        "print(\"\\nColunas totais:\")\n",
        "print(df.columns)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **ACHAR INFORMAÇOES ESPACIAIS**\n",
        "\n",
        "O conjunto de dados carece de Latitude e Longitude, o que é um impedimento crucial.\n",
        "\n",
        "A Solução é: Derivar o Lat/Long do REGION_ID usando o nome do município e a ferramenta Geopy.\n",
        "\n",
        "A Justificativa Principal é: Embora as features existentes já sejam climáticas históricas, a adição do Lat/Long não só oferece um contexto geográfico direto, mas, mais importante, serve como chave de agregação para incorporar dados climáticos externos mais ricos e detalhados (ex: interpolação de dados do INMET), maximizando assim o poder preditivo do modelo."
      ],
      "metadata": {
        "id": "mp_KZfw-KuVt"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57938312",
        "outputId": "883a4d32-d3f9-493f-ad7f-0b8cc305bdf4"
      },
      "source": [
        "# Instala a biblioteca geopy para geocodificação\n",
        "!pip install geopy\n",
        "!pip install unidecode"
      ],
      "execution_count": 89,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: geopy in /usr/local/lib/python3.12/dist-packages (2.4.1)\n",
            "Requirement already satisfied: geographiclib<3,>=1.52 in /usr/local/lib/python3.12/dist-packages (from geopy) (2.1)\n",
            "Requirement already satisfied: unidecode in /usr/local/lib/python3.12/dist-packages (1.4.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para melhorar a velocidade e para não ter o IP banido pela API, baixamos um .csv com a lat/long de todas as cidades brasileiras"
      ],
      "metadata": {
        "id": "_zGH5ZyBMQax"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Baixa o arquivo BR completo do GeoNames\n",
        "url = \"https://download.geonames.org/export/dump/BR.zip\"\n",
        "zip_path = \"BR.zip\"\n",
        "\n",
        "!wget -q {url} -O {zip_path}\n",
        "!unzip -o BR.zip\n",
        "\n",
        "# O arquivo útil chama-se BR.txt\n",
        "cols = [\n",
        "    \"geonameid\", \"name\", \"asciiname\", \"alternatenames\",\n",
        "    \"latitude\", \"longitude\", \"feature_class\", \"feature_code\",\n",
        "    \"country_code\", \"cc2\", \"admin1_code\", \"admin2_code\",\n",
        "    \"admin3_code\", \"admin4_code\", \"population\", \"elevation\",\n",
        "    \"dem\", \"timezone\", \"modification_date\"\n",
        "]\n",
        "\n",
        "df_geonames = pd.read_csv(\"BR.txt\", sep=\"\\t\", header=None, names=cols, dtype=str)\n",
        "\n",
        "# Filtrar apenas cidades oficiais (PPL = populated place)\n",
        "df_cities = df_geonames[df_geonames[\"feature_code\"] == \"PPLA\"]  # capitais municipais\n",
        "df_villages = df_geonames[df_geonames[\"feature_code\"] == \"PPL\"]  # municípios normais\n",
        "df_all = pd.concat([df_cities, df_villages])\n",
        "\n",
        "# Selecionar só o que importa\n",
        "df_final = df_all[[\"name\", \"latitude\", \"longitude\"]].copy()\n",
        "\n",
        "# Converter para float\n",
        "df_final[\"latitude\"] = df_final[\"latitude\"].astype(float)\n",
        "df_final[\"longitude\"] = df_final[\"longitude\"].astype(float)\n",
        "\n",
        "# Salvar CSV\n",
        "df_final.to_csv(\"municipios_brasil_lat_lon.csv\", index=False)\n",
        "\n",
        "print(\"Municípios carregados:\")\n",
        "print(df_final.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tQQhev_N7LiH",
        "outputId": "a03d3310-8ac4-46da-f98f-650817f2360a"
      },
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  BR.zip\n",
            "  inflating: readme.txt              \n",
            "  inflating: BR.txt                  \n",
            "Municípios carregados:\n",
            "           name  latitude  longitude\n",
            "1688   Teresina  -5.08917  -42.80194\n",
            "3552   São Luís  -2.52972  -44.30278\n",
            "5940     Recife  -8.05389  -34.88111\n",
            "9199      Natal  -5.79500  -35.20944\n",
            "11154    Maceió  -9.66583  -35.73528\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from difflib import get_close_matches\n",
        "import unicodedata\n",
        "\n",
        "# Carrega o CSV com as coordenadas dos municípios\n",
        "municipios_coords = pd.read_csv(\"municipios_brasil_lat_lon.csv\")\n",
        "\n",
        "# Função para normalizar texto (remover acentos, minúsculas)\n",
        "def normalize_text(text):\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    text = str(text).lower().strip()\n",
        "    # Remove acentos\n",
        "    text = unicodedata.normalize('NFD', text)\n",
        "    text = ''.join(char for char in text if unicodedata.category(char) != 'Mn')\n",
        "    return text\n",
        "\n",
        "# Normaliza os nomes dos municípios no CSV\n",
        "municipios_coords['name_normalized'] = municipios_coords['name'].apply(normalize_text)\n",
        "\n",
        "# Cria um dicionário para busca rápida\n",
        "coords_dict = dict(zip(\n",
        "    municipios_coords['name_normalized'],\n",
        "    zip(municipios_coords['latitude'], municipios_coords['longitude'])\n",
        "))\n",
        "\n",
        "def get_lat_lon(region_id):\n",
        "    try:\n",
        "        # Extrai o nome da região (removendo o ano e '_brasil_')\n",
        "        region_name = region_id.split('_brasil_')[0].replace('_', ' ').strip()\n",
        "        region_normalized = normalize_text(region_name)\n",
        "\n",
        "        # Estratégia 1: Busca exata\n",
        "        if region_normalized in coords_dict:\n",
        "            return coords_dict[region_normalized]\n",
        "\n",
        "        # Estratégia 2: Pega a última palavra (geralmente é o município)\n",
        "        last_word = region_normalized.split()[-1] if region_normalized else \"\"\n",
        "        if last_word and last_word in coords_dict:\n",
        "            return coords_dict[last_word]\n",
        "\n",
        "        # Estratégia 3: Testa cada palavra individualmente\n",
        "        words = region_normalized.split()\n",
        "        for word in reversed(words):  # começa do fim (mais específico)\n",
        "            if word in coords_dict:\n",
        "                return coords_dict[word]\n",
        "\n",
        "        # Estratégia 4: Busca aproximada (fuzzy matching)\n",
        "        matches = get_close_matches(region_normalized, coords_dict.keys(), n=1, cutoff=0.8)\n",
        "        if matches:\n",
        "            return coords_dict[matches[0]]\n",
        "\n",
        "        # Estratégia 5: Busca aproximada com a última palavra\n",
        "        if last_word:\n",
        "            matches = get_close_matches(last_word, coords_dict.keys(), n=1, cutoff=0.7)\n",
        "            if matches:\n",
        "                return coords_dict[matches[0]]\n",
        "\n",
        "        return None, None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Erro ao geocodificar {region_id}: {e}\")\n",
        "        return None, None\n",
        "\n",
        "# Aplica a função para obter as coordenadas\n",
        "df['new_lat'], df['new_lon'] = zip(*df['region_id'].apply(get_lat_lon))\n",
        "\n",
        "print(\"Valores únicos de new_lat e new_lon (amostra):\")\n",
        "print(df[['region_id', 'new_lat', 'new_lon']].head(10))\n",
        "print(f\"\\nNúmero de NAs em new_lat: {df['new_lat'].isnull().sum()}\")\n",
        "print(f\"Total de registros: {len(df)}\")\n",
        "\n",
        "# Debug: mostra os que falharam\n",
        "print(\"\\n=== Exemplos que falharam ===\")\n",
        "failed = df[df['new_lat'].isnull()]['region_id'].head(10)\n",
        "for region in failed:\n",
        "    region_name = region.split('_brasil_')[0].replace('_', ' ').strip()\n",
        "    print(f\"'{region_name}'\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaQPV5wL7mKU",
        "outputId": "5d7ff502-085e-4ac4-b1ee-9b5edc880f2f"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Valores únicos de new_lat e new_lon (amostra):\n",
            "                                     region_id   new_lat   new_lon\n",
            "0  triangulo mineiroalto paranaiba_brasil_2008 -19.67722 -51.19083\n",
            "1                            assis_brasil_2011  -5.00600 -40.84601\n",
            "2                       sul goiano_brasil_2007 -21.53722 -43.20167\n",
            "3    centro ocidental riograndense_brasil_2011 -27.23224 -52.02768\n",
            "4            oriental do tocantins_brasil_2011  -3.70757 -45.30524\n",
            "5            noroeste riograndense_brasil_2016 -10.66140 -38.99077\n",
            "6                 oeste paranaense_brasil_2006  -2.41062 -48.05416\n",
            "7   sudoeste de mato grosso do sul_brasil_2012 -12.95000 -39.26667\n",
            "8                     leste goiano_brasil_2014 -26.90393 -53.55319\n",
            "9             extremo oeste baiano_brasil_2012  -6.29253 -62.23404\n",
            "\n",
            "Número de NAs em new_lat: 0\n",
            "Total de registros: 247\n",
            "\n",
            "=== Exemplos que falharam ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "6ab13ea9",
        "outputId": "28c89609-cfab-4e1c-d2fe-c9d88e5147e0"
      },
      "source": [
        "# Atualiza as colunas 'lat' e 'lon' com as novas coordenadas\n",
        "df['lat'] = df['new_lat']\n",
        "df['lon'] = df['new_lon']\n",
        "\n",
        "# Remove as colunas temporárias 'new_lat' e 'new_lon'\n",
        "df = df.drop(columns=['new_lat', 'new_lon'])\n",
        "\n",
        "print(\"Colunas 'lat' e 'lon' atualizadas. Primeiras linhas:\")\n",
        "print(df[['lat', 'lon']].head())\n",
        "print(f\"\\nNúmero de NAs restantes em lat: {df['lat'].isnull().sum()}\")\n",
        "print(f\"Número de NAs restantes em lon: {df['lon'].isnull().sum()}\")"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colunas 'lat' e 'lon' atualizadas. Primeiras linhas:\n",
            "        lat       lon\n",
            "0 -19.67722 -51.19083\n",
            "1  -5.00600 -40.84601\n",
            "2 -21.53722 -43.20167\n",
            "3 -27.23224 -52.02768\n",
            "4  -3.70757 -45.30524\n",
            "\n",
            "Número de NAs restantes em lat: 0\n",
            "Número de NAs restantes em lon: 0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementaçao do Modelo"
      ],
      "metadata": {
        "id": "R1yDYWOFNNiJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikit-learn\n",
        "!pip install streamlit pyngrok"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "-HoxgpwXNYHK",
        "outputId": "a97cb9ec-94a7-4a05-8361-0a071ae71e51"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: streamlit in /usr/local/lib/python3.12/dist-packages (1.52.1)\n",
            "Requirement already satisfied: pyngrok in /usr/local/lib/python3.12/dist-packages (7.5.0)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.2.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.3.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<13,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (9.1.2)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.15.0)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Requirement already satisfied: pydeck<1,>=0.8.0b4 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.9.1)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.5.1)\n",
            "Requirement already satisfied: PyYAML>=5.1 in /usr/local/lib/python3.12/dist-packages (from pyngrok) (6.0.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2.12.0)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.11.12)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (3.0.3)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.37.0)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit) (0.29.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyngrok import ngrok\n",
        "\n",
        "# Coloque seu token aqui\n",
        "ngrok.set_auth_token(\"36RRGdxtdsr6l1S2mMDLnCrDNf2_2pxhtaKfNrdsZJAUEM8oq\")"
      ],
      "metadata": {
        "id": "cpCfRtrcSJF3"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "\n",
        "st.title(\"Teste Streamlit no Colab\")\n",
        "st.write(\"Se você está vendo isso, funcionou!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZabJe2VRYW_",
        "outputId": "02ae719b-1412-4bd8-8307-2dbecb539905"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_pickle(\"dados.pkl\")\n"
      ],
      "metadata": {
        "id": "Ux1QOWwJRU36"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
        "\n",
        "st.title(\"Modelos de Predição de Produtividade de Soja\")\n",
        "\n",
        "# --------------------------\n",
        "# Carregar DataFrame salvo\n",
        "# --------------------------\n",
        "@st.cache_data\n",
        "def load_data():\n",
        "    return pd.read_pickle(\"dados.pkl\")\n",
        "\n",
        "df = load_data()\n",
        "st.write(\"Dimensões do dataset:\", df.shape)\n",
        "st.write(df.head())\n",
        "\n",
        "# --------------------------\n",
        "# 1. Seleção do target\n",
        "# --------------------------\n",
        "available_targets = [\"yield\"]\n",
        "target_col = st.selectbox(\"Selecione o target\", available_targets, index=0)\n",
        "\n",
        "X = df.drop(columns=[target_col, 'region_id'])\n",
        "y = df[target_col]\n",
        "\n",
        "test_size = st.slider(\"Tamanho do conjunto de teste (%)\", 10, 50, 20) / 100\n",
        "random_state = st.number_input(\"Random State\", value=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=test_size, random_state=random_state\n",
        ")\n",
        "\n",
        "# --------------------------\n",
        "# 2. Seleção do Modelo\n",
        "# --------------------------\n",
        "st.subheader(\"Escolha o Algoritmo\")\n",
        "\n",
        "model_name = st.selectbox(\n",
        "    \"Modelo\",\n",
        "    [\"Linear Regression\", \"Random Forest\", \"Gradient Boosting\"]\n",
        ")\n",
        "\n",
        "# Hiperparâmetros\n",
        "if model_name == \"Random Forest\":\n",
        "    n_estimators = st.slider(\"Número de árvores\", 50, 500, 200)\n",
        "    max_depth = st.slider(\"Profundidade máxima\", 2, 30, 10)\n",
        "    model = RandomForestRegressor(\n",
        "        n_estimators=n_estimators,\n",
        "        max_depth=max_depth,\n",
        "        random_state=42\n",
        "    )\n",
        "\n",
        "elif model_name == \"Gradient Boosting\":\n",
        "    learning_rate = st.slider(\"Learning Rate\", 0.01, 0.5, 0.1)\n",
        "    n_estimators = st.slider(\"Número de estimadores\", 50, 500, 150)\n",
        "    model = GradientBoostingRegressor(\n",
        "        learning_rate=learning_rate,\n",
        "        n_estimators=n_estimators\n",
        "    )\n",
        "\n",
        "else:\n",
        "    model = LinearRegression()\n",
        "\n",
        "# --------------------------\n",
        "# 3. Treinar modelo\n",
        "# --------------------------\n",
        "if st.button(\"Treinar Modelo\"):\n",
        "    with st.spinner(\"Treinando...\"):\n",
        "        model.fit(X_train, y_train)\n",
        "        y_pred = model.predict(X_test)\n",
        "\n",
        "        # Métricas\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
        "        r2 = r2_score(y_test, y_pred)\n",
        "\n",
        "        st.success(\"Treinamento Concluído!\")\n",
        "\n",
        "        st.subheader(\"Métricas de Avaliação\")\n",
        "        st.metric(\"MAE\", f\"{mae:.3f}\")\n",
        "        st.metric(\"RMSE\", f\"{rmse:.3f}\")\n",
        "        st.metric(\"R²\", f\"{r2:.3f}\")\n",
        "\n",
        "        # --------------------------\n",
        "        # 4. Real vs Predito\n",
        "        # --------------------------\n",
        "        st.subheader(\"Real vs Predito (Linha)\")\n",
        "        df_plot = pd.DataFrame({\"Real\": y_test.values, \"Predito\": y_pred})\n",
        "        st.line_chart(df_plot)\n",
        "\n",
        "        # Scatter plot\n",
        "        st.subheader(\"Dispersão: Real vs Predito\")\n",
        "        fig, ax = plt.subplots()\n",
        "        ax.scatter(y_test, y_pred, alpha=0.6)\n",
        "        ax.set_xlabel(\"Real\")\n",
        "        ax.set_ylabel(\"Predito\")\n",
        "        ax.grid(True)\n",
        "        st.pyplot(fig)\n",
        "\n",
        "        # --------------------------\n",
        "        # 5. Erros (Resíduos)\n",
        "        # --------------------------\n",
        "        st.subheader(\"Distribuição dos Erros (Resíduos)\")\n",
        "        errors = y_test - y_pred\n",
        "        fig2, ax2 = plt.subplots()\n",
        "        ax2.hist(errors, bins=30)\n",
        "        ax2.set_xlabel(\"Erro (Real - Predito)\")\n",
        "        ax2.set_ylabel(\"Frequência\")\n",
        "        st.pyplot(fig2)\n",
        "\n",
        "        # --------------------------\n",
        "        # 6. Importância das Features\n",
        "        # --------------------------\n",
        "        if model_name in [\"Random Forest\", \"Gradient Boosting\"]:\n",
        "            st.subheader(\"Importância das Variáveis\")\n",
        "\n",
        "            importances = model.feature_importances_\n",
        "            feat_df = pd.DataFrame({\n",
        "                \"feature\": X.columns,\n",
        "                \"importance\": importances\n",
        "            }).sort_values(\"importance\", ascending=False)\n",
        "\n",
        "            st.bar_chart(feat_df.set_index(\"feature\"))\n",
        "\n",
        "            st.write(feat_df)\n",
        "\n",
        "        # --------------------------\n",
        "        # 7. Correlação das 10 features mais importantes\n",
        "        # --------------------------\n",
        "        st.subheader(\"Matriz de Correlação (Top 10 Features Importantes)\")\n",
        "\n",
        "        # Só funciona para modelos baseados em árvores\n",
        "        if model_name in [\"Random Forest\", \"Gradient Boosting\"]:\n",
        "            # Pega as 10 features mais importantes\n",
        "            feat_df_top10 = feat_df.head(10)\n",
        "            top10_cols = list(feat_df_top10[\"feature\"])\n",
        "\n",
        "            # Calcula a correlação apenas entre elas\n",
        "            corr_top10 = df[top10_cols].corr(numeric_only=True)\n",
        "\n",
        "            fig3, ax3 = plt.subplots(figsize=(8, 6))\n",
        "            cax = ax3.matshow(corr_top10)\n",
        "            fig3.colorbar(cax)\n",
        "\n",
        "            ax3.set_xticks(range(len(corr_top10.columns)))\n",
        "            ax3.set_yticks(range(len(corr_top10.columns)))\n",
        "            ax3.set_xticklabels(corr_top10.columns, rotation=90)\n",
        "            ax3.set_yticklabels(corr_top10.columns)\n",
        "\n",
        "            st.pyplot(fig3)\n",
        "\n",
        "        else:\n",
        "            st.info(\"A matriz de correlação baseada em importância só funciona para Random Forest ou Gradient Boosting.\")\n",
        "\n",
        "\n",
        "        # --------------------------\n",
        "        # 8. Piores e Melhores Previsões\n",
        "        # --------------------------\n",
        "        st.subheader(\"Erros Individuais\")\n",
        "        df_errors = pd.DataFrame({\n",
        "            \"Real\": y_test,\n",
        "            \"Predito\": y_pred,\n",
        "            \"Erro\": errors\n",
        "        })\n",
        "\n",
        "        st.write(\"20 piores erros:\")\n",
        "        st.write(df_errors.sort_values(\"Erro\").head(20))\n",
        "\n",
        "        st.write(\"20 melhores acertos:\")\n",
        "        st.write(df_errors.sort_values(\"Erro\", key=abs).head(20))\n",
        "\n",
        "        # --------------------------\n",
        "        # 9. Download dos resultados\n",
        "        # --------------------------\n",
        "        st.subheader(\"Download\")\n",
        "\n",
        "        csv = df_errors.to_csv(index=False).encode(\"utf-8\")\n",
        "        st.download_button(\n",
        "            label=\"⬇️ Baixar CSV de Resultados\",\n",
        "            data=csv,\n",
        "            file_name=\"resultados_modelo.csv\",\n",
        "            mime=\"text/csv\"\n",
        "        )\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "3qlaLNB7NTis",
        "outputId": "53cb1b88-a7a9-4dd5-d2f2-e3d52f9b79dd"
      },
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "public_url = ngrok.connect(addr=\"8050\")\n",
        "print(\"Acesse aqui:\", public_url)\n",
        "\n",
        "!streamlit run app.py --server.port 8050"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ls7TYMnITAb0",
        "outputId": "ecf1e2ab-dafa-409b-d5a4-84af109cdb13",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acesse aqui: NgrokTunnel: \"https://presecular-yael-unavowably.ngrok-free.dev\" -> \"http://localhost:8050\"\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8050\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8050\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.123.91.119:8050\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ]
    }
  ]
}